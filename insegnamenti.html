<!DOCTYPE HTML SYSTEM "html.dtd">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/> 
<BODY>
<HTML><HEAD><TITLE>
Aldo Solari - personal webpage
</TITLE>
<SCRIPT SRC="/js/index.js"> </SCRIPT>
</HEAD>

<BODY>

<H1> Insegnamenti  </H1>


<p><span style = "font-size: 20px; font-weight: bold;  ">Laurea Triennale</span></p>

Università degli Studi di Milano-Bicocca, Corso di Laurea Triennale in Scienze Statistiche ed Economiche.
</UL><UL><LI> 
<A href="https://aldosolari.github.io/AE/">Analisi Esplorativa</A>. (42 ore).
</UL>

<p><span style = "font-size: 20px; font-weight: bold;  ">Laurea Magistrale</span></p>

Università degli Studi di Milano-Bicocca, Corso di Laurea Magistrale in Scienze Statistiche ed Economiche.

</UL><UL><LI> 
<A href="https://aldosolari.github.io/DM/">Data Mining</A>. (42 ore).<br>
</UL><UL><LI> 
<A href="https://aldosolari.github.io/MI/">Statistical Learning</A>. (21 ore).<br>
</UL><UL><LI>  <b>Lab Data Challenges</b>. (24 ore).
</UL>

<p><span style = "font-size: 20px; font-weight: bold;  ">Dottorato</span></p>

Università degli Studi di Milano-Bicocca, Dottorato di Ricerca in Economia e Statistica.

</UL><UL><LI> 
<b>Statistical Inference II</b>. (15 ore).<br>


</UL><UL><LI>
<b>Statistical Learning III</b>. (8 ore).<br></UL>


<H1> Tesi  </H1>

Per informazioni riguardanti la tesi, si prega di scrivere una mail a aldo.solari@unimib.it 

Per la stesura della tesi, è obbligatorio l’utilizzo del linguaggio di markup LaTeX. 

Viene reso disponibile un <A href="https://aldosolari.github.io/templatetesi.zip">template</A> (preparato da Tommaso Rigon, che ringrazio). Gli argomenti di tesi vengono aggiornati regolarmente. Si prega di consultare questa pagina in caso di interesse. <br>

</UL><UL><LI>
<b>Conformal inference</b> permette di quantificare l'incertezza delle previsioni ottenute da algoritmi di machine learning. 
Per un approfondimento, si prenda visione del seguente <A href="https://sites.google.com/berkeley.edu/dfuq21/tutorial-resources">tutorial</A>. </UL>

</UL><UL><LI>
<b>Selective inference</b> permette di esplorare i dati fornendo garanzie inferenziali che tengono conto del processo di selezione.
Per un approfondimento, si prenda visione dei seguenti <A href="https://sites.google.com/view/selective-inference-seminar">interventi</A>. </UL>


</UL><UL><LI>
<b>Hot hand fallacy</b> (illusione della mano calda): intuizione contro analisi.
Per un approfondimento, si prenda visione del seguente <A href="http://www.restud.com/wp-content/uploads/2021/04/hot_hand.pdf">manoscritto</A>. </UL>

</UL><UL><LI>
<b>Prediction Error Estimation</b>.
Per un approfondimento, si prenda visione <A href="https://arxiv.org/abs/2104.00673">qui</A> e <A href="https://arxiv.org/abs/1704.08160">qua</A>. </UL>


</UL><UL><LI>
<b>Statistical learning methods</b>. Ad esempio, James–Stein Estimation and Ridge Regression, Sparse Modeling and the Lasso, Random Forests and Boosting, Support-Vector Machines and Kernel Methods, Singular Value Decomposition and Matrix completion, Nonparametric Regression, etc. Per un approfondimento, si prenda visione dei libri <A href="https://web.stanford.edu/~hastie/ElemStatLearn/3">The Elements of Statistical Learning</A>, <A href="https://web.stanford.edu/~hastie/CASI/">Computer-Age Statistical Inference</A>, <A href="http://www.stat.cmu.edu/~larry/all-of-nonpar/index.html">All of Nonparametric Statistics
</A>, <A href="https://web.stanford.edu/~hastie/StatLearnSparsity/">Statistical Learning with Sparsity </A>.</UL> 

</UL><UL><LI>
<b>Hypothesis Testing</b>. Per un approfondimento, si prenda visione dei libri 
<A href="https://www.taylorfrancis.com/books/mono/10.1201/b14832/theoretical-statistics-cox-hinkley">Theoretical Statistics</A>, <A href="https://www.springer.com/gp/book/9780387988641">Testing Statistical Hypotheses</A>, <A href="https://www.cambridge.org/core/books/principles-of-statistical-inference/BCD3734047D403DF5352EA58F41D3181">Principles of Statistical Inference</A>.</UL> 

</UL><UL><LI>
<b>Variable Selection with statistical guarantees</b>. Stability Selection, Post-Selection Inference, the Knockoff Filter, Leave-One-Covariate-Out (LOCO) Inference, etc. Per un approfondimento, si prenda visione 
<A href="https://web.stanford.edu/group/candes/knockoffs/">qui</A> e <A href="http://www.stat.cmu.edu/~ryantibs/talks/loco-2018.pdf">qua</A>. </UL>



</BODY></HTML>